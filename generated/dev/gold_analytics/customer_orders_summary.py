# Generated by LakehousePlumber
# Pipeline: gold_analytics
# FlowGroup: customer_orders_summary

from pyspark import pipelines as dp
from pyspark.sql import DataFrame

# Pipeline Configuration
PIPELINE_ID = "gold_analytics"
FLOWGROUP_ID = "customer_orders_summary"


# ============================================================================
# SOURCE VIEWS
# ============================================================================


@dp.temporary_view()
def v_customer_dim():
    """Delta source: spalyam_catalog.silver.customer_dim"""
    df = spark.read.table("spalyam_catalog.silver.customer_dim")

    return df


@dp.temporary_view()
def v_orders_fct():
    """Delta source: spalyam_catalog.silver.orders_fct"""
    df = spark.read.table("spalyam_catalog.silver.orders_fct")

    return df


# ============================================================================
# TRANSFORMATION VIEWS
# ============================================================================


@dp.temporary_view(comment="SQL transform: customer_orders_agg")
def v_customer_orders_summary():
    """SQL transform: customer_orders_agg"""
    df = spark.sql(
        """SELECT
  c.customer_id,
  c.first_name,
  c.last_name,
  c.email,
  c.city,
  c.state,
  c.country,
  COUNT(o.order_id) as total_orders,
  SUM(o.quantity * o.unit_price) as total_revenue,
  AVG(o.quantity * o.unit_price) as avg_order_value,
  MIN(o.order_date) as first_order_date,
  MAX(o.order_date) as last_order_date
FROM v_customer_dim c
LEFT JOIN v_orders_fct o ON c.customer_id = o.customer_id
GROUP BY
  c.customer_id, c.first_name, c.last_name, c.email,
  c.city, c.state, c.country"""
    )

    return df


# ============================================================================
# TARGET TABLES
# ============================================================================


@dp.materialized_view(
    name="spalyam_catalog.gold.customer_orders_summary",
    comment="Materialized view: customer_orders_summary",
    table_properties={},
)
def customer_orders_summary():
    """Write to spalyam_catalog.gold.customer_orders_summary from multiple sources"""
    # Materialized views use batch processing
    df = spark.read.table("v_customer_orders_summary")

    return df
