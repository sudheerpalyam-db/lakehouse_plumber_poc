# Generated by LakehousePlumber
# Pipeline: gold_analytics
# FlowGroup: product_sales_summary

from pyspark import pipelines as dp
from pyspark.sql import DataFrame

# Pipeline Configuration
PIPELINE_ID = "gold_analytics"
FLOWGROUP_ID = "product_sales_summary"


# ============================================================================
# SOURCE VIEWS
# ============================================================================


@dp.temporary_view()
def v_products():
    """Delta source: spalyam_catalog.bronze.products"""
    df = spark.read.table("spalyam_catalog.bronze.products")

    return df


@dp.temporary_view()
def v_orders_for_products():
    """Delta source: spalyam_catalog.silver.orders_fct"""
    df = spark.read.table("spalyam_catalog.silver.orders_fct")

    return df


# ============================================================================
# TRANSFORMATION VIEWS
# ============================================================================


@dp.temporary_view(comment="SQL transform: product_sales_agg")
def v_product_sales_summary():
    """SQL transform: product_sales_agg"""
    df = spark.sql(
        """SELECT
  p.product_id,
  p.product_name,
  p.category,
  p.price as list_price,
  COUNT(o.order_id) as times_ordered,
  SUM(o.quantity) as total_quantity_sold,
  SUM(o.quantity * o.unit_price) as total_revenue,
  AVG(o.unit_price) as avg_selling_price
FROM v_products p
LEFT JOIN v_orders_for_products o ON p.product_id = o.product_id
GROUP BY
  p.product_id, p.product_name, p.category, p.price"""
    )

    return df


# ============================================================================
# TARGET TABLES
# ============================================================================


@dp.materialized_view(
    name="spalyam_catalog.gold.product_sales_summary",
    comment="Materialized view: product_sales_summary",
    table_properties={},
)
def product_sales_summary():
    """Write to spalyam_catalog.gold.product_sales_summary from multiple sources"""
    # Materialized views use batch processing
    df = spark.read.table("v_product_sales_summary")

    return df
