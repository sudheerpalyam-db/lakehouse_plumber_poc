# Generated by LakehousePlumber - Bundle Resource for raw_ingestions

resources:
  pipelines:
    raw_ingestions_pipeline:
      name: raw_ingestions_pipeline
      
      # Catalog and Schema for the pipeline
      # Using databricks.yml variables (not defined in pipeline config)
      catalog: ${var.default_pipeline_catalog}
      schema: ${var.default_pipeline_schema}
      
      serverless: true
      libraries:
        - glob:
            include: ${workspace.file_path}/generated/${bundle.target}/raw_ingestions/**
      
      root_path: ${workspace.file_path}/generated/${bundle.target}/raw_ingestions
      
      configuration:
        bundle.sourcePath: ${workspace.file_path}/generated/${bundle.target}      
      channel: CURRENT
      # Additional pipeline configuration options 
      # Add to your pipeline_config-<env>.yaml as needed and pass the file path through the --pipeline-config flag:
      # You can use substitutions file to define the values for the pipeline configuration using the following syntax:
      # {variable_name}
 
      # Compute clusters configuration (alternative to serverless)
      # clusters:
      #   - label: default
      #     node_type_id: Standard_D16ds_v5
      #     driver_node_type_id: Standard_D32ds_v5
      #     policy_id: 1234ABCD1234ABCD
      #     autoscale:
      #       min_workers: 1
      #       max_workers: 5
      #       mode: ENHANCED
      
      # Enable continuous processing
      # continuous: false
      
   
      # Enable Photon engine only for classic computer not serverless
      # photon: true
      
      # DLT edition (CORE, PRO, ADVANCED)
      # edition: ADVANCED
      
      # Runtime channel (CURRENT, PREVIEW)
      # channel: CURRENT
      
      # Notification settings
      # notifications:
      #   - email_recipients:
      #       - user@databricks.com
      #     alerts:
      #       - on-update-success
      #       - on-update-failure
      #       - on-update-fatal-failure
      #       - on-flow-failure
      
      # Custom tags only for classic computer not serverless (serverless has its own tags through compute policy)
      # tags:
      #   tag1: val1
      
      # Event log configuration
      # event_log:
      #   name: pipeline_evenlog
      #   schema: _meta
      #   catalog: acmi_edw_dev

      # permissions:
      #   - service_principal_name: 2aa4ed8e-0a18-4072-97c6-9c074c8be40d
      #     level: CAN_MANAGE
      #   - user_name: user@example.com
      #     level: CAN_RUN
      #   - group_name: data-engineers 
      #     level: CAN_VIEW
      
      # # Pipeline-level run_as
      # run_as:
      #   service_principal_name: 2aa4ed8e-0a18-4072-97c6-9c074c8be40d OR "{SP_NAME}" (if using substitutions file)