# Gold Analytics Pipeline
# Generated by Lakehouse Plumber - DO NOT EDIT MANUALLY
# This file is auto-generated from YAML configurations

from pyspark.sql import functions as F
import dlt

# ==============================================================================
# Pipeline Configuration
# ==============================================================================
PIPELINE_ID = "gold_analytics"
CATALOG = spark.conf.get("catalog", "spalyam_catalog")
BRONZE_SCHEMA = spark.conf.get("bronze_schema", "bronze")
SILVER_SCHEMA = spark.conf.get("silver_schema", "silver")
GOLD_SCHEMA = spark.conf.get("gold_schema", "gold")

# ==============================================================================
# FLOWGROUP: sales_analytics
# ==============================================================================


@dlt.view()
def v_orders_for_sales():
    """Read enriched orders from silver layer for sales analytics"""
    return spark.read.table(f"{CATALOG}.{SILVER_SCHEMA}.orders_enriched")


@dlt.view()
def v_daily_sales_summary():
    """Aggregate daily sales by category"""
    df = dlt.read("v_orders_for_sales")
    return df.groupBy(
        F.date_trunc("day", "order_date").alias("sale_date"),
        "product_category"
    ).agg(
        F.countDistinct("order_id").alias("total_orders"),
        F.countDistinct("customer_id").alias("unique_customers"),
        F.sum("quantity").alias("total_units_sold"),
        F.sum("total_amount").alias("total_revenue"),
        F.avg("total_amount").alias("avg_order_value"),
        F.min("total_amount").alias("min_order_value"),
        F.max("total_amount").alias("max_order_value")
    )


@dlt.table(
    name="daily_sales_summary",
    comment="Gold layer: Daily sales summary by product category",
    table_properties={
        "delta.autoOptimize.optimizeWrite": "true",
    },
)
def daily_sales_summary():
    """Daily sales summary materialized view"""
    return dlt.read("v_daily_sales_summary")


# ==============================================================================
# FLOWGROUP: customer_analytics
# ==============================================================================


@dlt.view()
def v_customers_for_360():
    """Read customers from silver layer for Customer 360"""
    return spark.read.table(f"{CATALOG}.{SILVER_SCHEMA}.customers")


@dlt.view()
def v_orders_for_360():
    """Read enriched orders from silver layer for Customer 360"""
    return spark.read.table(f"{CATALOG}.{SILVER_SCHEMA}.orders_enriched")


@dlt.view()
def v_customer_360():
    """Create Customer 360 view with lifetime metrics"""
    customers_df = dlt.read("v_customers_for_360")
    orders_df = dlt.read("v_orders_for_360")

    # Calculate customer order metrics
    customer_orders = orders_df.groupBy("customer_id").agg(
        F.countDistinct("order_id").alias("lifetime_orders"),
        F.sum("total_amount").alias("lifetime_revenue"),
        F.avg("total_amount").alias("avg_order_value"),
        F.min("order_date").alias("first_order_date"),
        F.max("order_date").alias("last_order_date"),
        F.countDistinct("product_category").alias("categories_purchased")
    ).withColumn(
        "customer_tenure_days",
        F.datediff(F.col("last_order_date"), F.col("first_order_date"))
    ).withColumn(
        "customer_segment",
        F.when(F.col("lifetime_revenue") >= 10000, "VIP")
        .when(F.col("lifetime_revenue") >= 5000, "Gold")
        .when(F.col("lifetime_revenue") >= 1000, "Silver")
        .otherwise("Bronze")
    ).withColumn(
        "engagement_status",
        F.when(F.datediff(F.current_date(), F.col("last_order_date")) <= 30, "Active")
        .when(F.datediff(F.current_date(), F.col("last_order_date")) <= 90, "At Risk")
        .otherwise("Churned")
    )

    # Join customers with their order metrics
    result = customers_df.alias("c").join(
        customer_orders.alias("co"),
        F.col("c.customer_id") == F.col("co.customer_id"),
        "left"
    ).select(
        F.col("c.customer_id"),
        F.col("c.first_name"),
        F.col("c.last_name"),
        F.col("c.email"),
        F.col("c.city"),
        F.col("c.state"),
        F.col("c.country"),
        F.col("c.created_date").alias("customer_since"),
        F.coalesce(F.col("co.lifetime_orders"), F.lit(0)).alias("lifetime_orders"),
        F.coalesce(F.col("co.lifetime_revenue"), F.lit(0)).alias("lifetime_revenue"),
        F.coalesce(F.col("co.avg_order_value"), F.lit(0)).alias("avg_order_value"),
        F.col("co.first_order_date"),
        F.col("co.last_order_date"),
        F.coalesce(F.col("co.customer_tenure_days"), F.lit(0)).alias("customer_tenure_days"),
        F.coalesce(F.col("co.categories_purchased"), F.lit(0)).alias("categories_purchased"),
        F.coalesce(F.col("co.customer_segment"), F.lit("New")).alias("customer_segment"),
        F.coalesce(F.col("co.engagement_status"), F.lit("New")).alias("engagement_status"),
        F.current_timestamp().alias("_gold_processing_timestamp")
    )

    return result


@dlt.table(
    name="customer_360",
    comment="Gold layer: Customer 360 view with lifetime metrics and segmentation",
    table_properties={
        "delta.autoOptimize.optimizeWrite": "true",
    },
)
def customer_360():
    """Customer 360 materialized view"""
    return dlt.read("v_customer_360")


# ==============================================================================
# FLOWGROUP: product_analytics
# ==============================================================================


@dlt.view()
def v_products_for_perf():
    """Read products from bronze layer for performance analytics"""
    return spark.read.table(f"{CATALOG}.{BRONZE_SCHEMA}.products")


@dlt.view()
def v_orders_for_perf():
    """Read enriched orders from silver layer for product performance"""
    return spark.read.table(f"{CATALOG}.{SILVER_SCHEMA}.orders_enriched")


@dlt.view()
def v_product_performance():
    """Create product performance metrics"""
    products_df = dlt.read("v_products_for_perf")
    orders_df = dlt.read("v_orders_for_perf")

    # Calculate product sales metrics
    product_sales = orders_df.groupBy("product_id").agg(
        F.countDistinct("order_id").alias("total_orders"),
        F.sum("quantity").alias("total_units_sold"),
        F.sum("total_amount").alias("total_revenue"),
        F.countDistinct("customer_id").alias("unique_buyers"),
        F.avg("quantity").alias("avg_units_per_order"),
        F.min("order_date").alias("first_sale_date"),
        F.max("order_date").alias("last_sale_date")
    )

    # Add rankings
    from pyspark.sql.window import Window
    revenue_window = Window.orderBy(F.desc("total_revenue"))
    units_window = Window.orderBy(F.desc("total_units_sold"))

    product_sales_ranked = product_sales.withColumn(
        "revenue_rank", F.rank().over(revenue_window)
    ).withColumn(
        "units_rank", F.rank().over(units_window)
    ).withColumn(
        "revenue_percentile", F.percent_rank().over(revenue_window)
    )

    # Join with products
    result = products_df.alias("p").join(
        product_sales_ranked.alias("ps"),
        F.col("p.product_id") == F.col("ps.product_id"),
        "left"
    ).select(
        F.col("p.product_id"),
        F.col("p.product_name"),
        F.col("p.category"),
        F.col("p.price").alias("list_price"),
        F.col("p.stock_quantity").alias("current_stock"),
        F.coalesce(F.col("ps.total_orders"), F.lit(0)).alias("total_orders"),
        F.coalesce(F.col("ps.total_units_sold"), F.lit(0)).alias("total_units_sold"),
        F.coalesce(F.col("ps.total_revenue"), F.lit(0)).alias("total_revenue"),
        F.coalesce(F.col("ps.unique_buyers"), F.lit(0)).alias("unique_buyers"),
        F.coalesce(F.col("ps.avg_units_per_order"), F.lit(0)).alias("avg_units_per_order"),
        F.col("ps.first_sale_date"),
        F.col("ps.last_sale_date"),
        F.coalesce(F.col("ps.revenue_rank"), F.lit(0)).alias("revenue_rank"),
        F.coalesce(F.col("ps.units_rank"), F.lit(0)).alias("units_rank"),
        F.when(F.col("ps.revenue_percentile") <= 0.1, "Top 10%")
        .when(F.col("ps.revenue_percentile") <= 0.25, "Top 25%")
        .when(F.col("ps.revenue_percentile") <= 0.5, "Top 50%")
        .otherwise("Bottom 50%").alias("performance_tier"),
        F.when(F.col("p.stock_quantity") == 0, "Out of Stock")
        .when(F.col("p.stock_quantity") < 10, "Low Stock")
        .when(F.col("p.stock_quantity") < 50, "Medium Stock")
        .otherwise("Well Stocked").alias("stock_status"),
        F.current_timestamp().alias("_gold_processing_timestamp")
    )

    return result


@dlt.table(
    name="product_performance",
    comment="Gold layer: Product performance analytics with rankings and stock status",
    table_properties={
        "delta.autoOptimize.optimizeWrite": "true",
    },
)
def product_performance():
    """Product performance materialized view"""
    return dlt.read("v_product_performance")
