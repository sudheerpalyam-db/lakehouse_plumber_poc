# Bronze Ingestion Pipeline
# Generated by Lakehouse Plumber - DO NOT EDIT MANUALLY
# This file is auto-generated from YAML configurations

from pyspark.sql import functions as F
import dlt

# ==============================================================================
# Pipeline Configuration
# ==============================================================================
PIPELINE_ID = "bronze_ingestion"
CATALOG = spark.conf.get("catalog", "spalyam_catalog")
BRONZE_SCHEMA = spark.conf.get("bronze_schema", "bronze")
LANDING_VOLUME = spark.conf.get("landing_volume", f"/Volumes/{CATALOG}/raw_data/landing")

# ==============================================================================
# FLOWGROUP: customer_ingestion
# Template: csv_ingestion_template
# ==============================================================================

# Schema hints for customers CSV
customers_schema_hints = "customer_id STRING, first_name STRING, last_name STRING, email STRING, phone STRING, address STRING, city STRING, state STRING, zip_code STRING, country STRING, created_date STRING"


@dlt.view()
def v_customers_raw():
    """Load customer CSV files from landing zone"""
    df = (
        spark.readStream.format("cloudFiles")
        .option("cloudFiles.format", "csv")
        .option("header", "true")
        .option("delimiter", ",")
        .option("cloudFiles.schemaEvolutionMode", "addNewColumns")
        .option("cloudFiles.inferColumnTypes", "true")
        .option("cloudFiles.schemaHints", customers_schema_hints)
        .load(f"{LANDING_VOLUME}/customers/*.csv")
    )
    return df


@dlt.view()
def v_customers_enriched():
    """Add operational metadata to customers"""
    df = dlt.readStream("v_customers_raw")

    # Add operational metadata columns
    df = df.withColumn("_record_hash", F.xxhash64(*[F.col(c) for c in df.columns]))
    df = df.withColumn("_source_file_path", F.col("_metadata.file_path"))
    df = df.withColumn("_source_file_name", F.col("_metadata.file_name"))
    df = df.withColumn("_source_file_size", F.col("_metadata.file_size"))
    df = df.withColumn(
        "_source_file_modification_time", F.col("_metadata.file_modification_time")
    )
    df = df.withColumn("_processing_timestamp", F.current_timestamp())

    return df


@dlt.table(
    name="customers",
    comment="Bronze layer: customers streaming table ingested from CSV",
    table_properties={
        "delta.autoOptimize.optimizeWrite": "true",
        "delta.enableChangeDataFeed": "true",
    },
)
def customers():
    """Customers bronze table"""
    return dlt.readStream("v_customers_enriched")


# ==============================================================================
# FLOWGROUP: orders_ingestion
# Template: csv_ingestion_template
# ==============================================================================

# Schema hints for orders CSV
orders_schema_hints = "order_id STRING, customer_id STRING, product_id STRING, quantity INT, unit_price DECIMAL(10,2), order_date STRING, status STRING"


@dlt.view()
def v_orders_raw():
    """Load order CSV files from landing zone"""
    df = (
        spark.readStream.format("cloudFiles")
        .option("cloudFiles.format", "csv")
        .option("header", "true")
        .option("delimiter", ",")
        .option("cloudFiles.schemaEvolutionMode", "addNewColumns")
        .option("cloudFiles.inferColumnTypes", "true")
        .option("cloudFiles.schemaHints", orders_schema_hints)
        .load(f"{LANDING_VOLUME}/orders/*.csv")
    )
    return df


@dlt.view()
def v_orders_enriched():
    """Add operational metadata to orders"""
    df = dlt.readStream("v_orders_raw")

    # Add operational metadata columns
    df = df.withColumn("_record_hash", F.xxhash64(*[F.col(c) for c in df.columns]))
    df = df.withColumn("_source_file_path", F.col("_metadata.file_path"))
    df = df.withColumn("_source_file_name", F.col("_metadata.file_name"))
    df = df.withColumn("_source_file_size", F.col("_metadata.file_size"))
    df = df.withColumn(
        "_source_file_modification_time", F.col("_metadata.file_modification_time")
    )
    df = df.withColumn("_processing_timestamp", F.current_timestamp())

    return df


@dlt.table(
    name="orders",
    comment="Bronze layer: orders streaming table ingested from CSV",
    table_properties={
        "delta.autoOptimize.optimizeWrite": "true",
        "delta.enableChangeDataFeed": "true",
    },
)
def orders():
    """Orders bronze table"""
    return dlt.readStream("v_orders_enriched")


# ==============================================================================
# FLOWGROUP: products_ingestion
# Full action definition (not using template)
# ==============================================================================

# Schema hints for products CSV
products_schema_hints = "product_id STRING, product_name STRING, category STRING, price DECIMAL(10,2), stock_quantity INT, supplier_id STRING"


@dlt.view()
def v_products_raw():
    """Load product CSV files from landing zone"""
    df = (
        spark.readStream.format("cloudFiles")
        .option("cloudFiles.format", "csv")
        .option("header", "true")
        .option("delimiter", ",")
        .option("cloudFiles.schemaEvolutionMode", "addNewColumns")
        .option("cloudFiles.inferColumnTypes", "true")
        .option("cloudFiles.schemaHints", products_schema_hints)
        .load(f"{LANDING_VOLUME}/products/*.csv")
    )
    return df


@dlt.view()
@dlt.expect_all_or_fail({"valid_product_id": "product_id IS NOT NULL"})
@dlt.expect_or_drop("valid_product_name", "product_name IS NOT NULL")
@dlt.expect_or_drop("valid_price", "price >= 0 AND price <= 999999.99")
def v_products_validated():
    """Validate product data with expectations"""
    return dlt.readStream("v_products_raw")


@dlt.view()
def v_products_enriched():
    """Add operational metadata to products"""
    df = dlt.readStream("v_products_validated")

    # Add operational metadata columns
    df = df.withColumn("_record_hash", F.xxhash64(*[F.col(c) for c in df.columns]))
    df = df.withColumn("_source_file_path", F.col("_metadata.file_path"))
    df = df.withColumn("_processing_timestamp", F.current_timestamp())

    return df


@dlt.table(
    name="products",
    comment="Bronze layer: products streaming table ingested from CSV",
    table_properties={
        "delta.autoOptimize.optimizeWrite": "true",
        "delta.enableChangeDataFeed": "true",
    },
)
def products():
    """Products bronze table"""
    return dlt.readStream("v_products_enriched")
