# Silver Transforms Pipeline
# Generated by Lakehouse Plumber - DO NOT EDIT MANUALLY
# This file is auto-generated from YAML configurations

from pyspark.sql import functions as F
import dlt

# ==============================================================================
# Pipeline Configuration
# ==============================================================================
PIPELINE_ID = "silver_transforms"
CATALOG = spark.conf.get("catalog", "spalyam_catalog")
BRONZE_SCHEMA = spark.conf.get("bronze_schema", "bronze")
SILVER_SCHEMA = spark.conf.get("silver_schema", "silver")

# ==============================================================================
# FLOWGROUP: customer_transforms
# ==============================================================================


@dlt.view()
def v_customers_bronze():
    """Read customers from bronze layer"""
    return spark.readStream.table(f"{CATALOG}.{BRONZE_SCHEMA}.customers")


@dlt.view()
def v_customers_cleaned():
    """Clean and standardize customer data"""
    df = dlt.readStream("v_customers_bronze")
    return df.selectExpr(
        "customer_id",
        "TRIM(UPPER(first_name)) AS first_name",
        "TRIM(UPPER(last_name)) AS last_name",
        "LOWER(TRIM(email)) AS email",
        "REGEXP_REPLACE(phone, '[^0-9]', '') AS phone_normalized",
        "TRIM(address) AS address",
        "TRIM(UPPER(city)) AS city",
        "TRIM(UPPER(state)) AS state",
        "TRIM(zip_code) AS zip_code",
        "TRIM(UPPER(country)) AS country",
        "CAST(created_date AS TIMESTAMP) AS created_date",
        "_record_hash",
        "_source_file_path",
        "_processing_timestamp AS bronze_processing_timestamp"
    ).filter("customer_id IS NOT NULL")


@dlt.view()
def v_customers_silver():
    """Add silver layer audit columns to customers"""
    df = dlt.readStream("v_customers_cleaned")

    # Add silver layer metadata
    df = df.withColumn(
        "_silver_record_hash", F.xxhash64(*[F.col(c) for c in df.columns])
    )
    df = df.withColumn("_silver_processing_timestamp", F.current_timestamp())

    return df


@dlt.view()
@dlt.expect("valid_customer_id", "customer_id IS NOT NULL")
@dlt.expect_or_drop(
    "valid_email_format",
    "email RLIKE '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$'"
)
def v_customers_validated():
    """Validate customer data with expectations"""
    return dlt.readStream("v_customers_silver")


@dlt.table(
    name="customers",
    comment="Silver layer: cleaned and validated customers",
    table_properties={
        "delta.autoOptimize.optimizeWrite": "true",
        "delta.enableChangeDataFeed": "true",
        "delta.deletedFileRetentionDuration": "7 days",
    },
)
def customers():
    """Silver customers table"""
    return dlt.readStream("v_customers_validated")


# ==============================================================================
# FLOWGROUP: orders_transforms
# ==============================================================================


@dlt.view()
def v_orders_bronze():
    """Read orders from bronze layer"""
    return spark.readStream.table(f"{CATALOG}.{BRONZE_SCHEMA}.orders")


@dlt.view()
def v_products_dim():
    """Read products dimension for enrichment (batch)"""
    return spark.read.table(f"{CATALOG}.{BRONZE_SCHEMA}.products")


@dlt.view()
def v_orders_enriched():
    """Enrich orders with product data"""
    orders_df = dlt.readStream("v_orders_bronze")
    products_df = dlt.read("v_products_dim")

    # Join orders with products
    enriched = orders_df.alias("o").join(
        products_df.alias("p"),
        F.col("o.product_id") == F.col("p.product_id"),
        "left"
    ).select(
        F.col("o.order_id"),
        F.col("o.customer_id"),
        F.col("o.product_id"),
        F.col("p.product_name"),
        F.col("p.category").alias("product_category"),
        F.col("o.quantity").cast("int").alias("quantity"),
        F.col("o.unit_price").cast("decimal(10,2)").alias("unit_price"),
        (F.col("o.quantity").cast("int") * F.col("o.unit_price").cast("decimal(10,2)")).alias("total_amount"),
        F.col("o.order_date").cast("timestamp").alias("order_date"),
        F.col("o.status").alias("order_status"),
        F.col("o._record_hash").alias("bronze_record_hash"),
        F.col("o._processing_timestamp").alias("bronze_processing_timestamp")
    )

    return enriched


@dlt.view()
def v_orders_silver():
    """Add silver layer audit columns to orders"""
    df = dlt.readStream("v_orders_enriched")

    # Add silver layer metadata
    df = df.withColumn(
        "_silver_record_hash", F.xxhash64(*[F.col(c) for c in df.columns])
    )
    df = df.withColumn("_silver_processing_timestamp", F.current_timestamp())

    return df


@dlt.view()
@dlt.expect("valid_order_id", "order_id IS NOT NULL")
@dlt.expect_or_drop("valid_quantity", "quantity >= 1 AND quantity <= 10000")
@dlt.expect_or_drop("valid_total_amount", "total_amount >= 0")
def v_orders_validated():
    """Validate order data with expectations"""
    return dlt.readStream("v_orders_silver")


@dlt.table(
    name="orders_enriched",
    comment="Silver layer: enriched orders with product data",
    table_properties={
        "delta.autoOptimize.optimizeWrite": "true",
        "delta.enableChangeDataFeed": "true",
    },
)
def orders_enriched():
    """Silver orders enriched table"""
    return dlt.readStream("v_orders_validated")
