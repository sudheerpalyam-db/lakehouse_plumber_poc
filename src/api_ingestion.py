# API Ingestion Pipeline
# Ingests data from JSONPlaceholder REST API
# Generated by Lakehouse Plumber framework

import dlt
from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, DoubleType
import requests
import json

# ==============================================================================
# Pipeline Configuration
# ==============================================================================
PIPELINE_ID = "api_ingestion"
CATALOG = spark.conf.get("catalog", "spalyam_catalog")
BRONZE_SCHEMA = spark.conf.get("bronze_schema", "bronze")

# API Configuration
API_BASE_URL = "https://jsonplaceholder.typicode.com"

# ==============================================================================
# Helper Function: Fetch data from REST API
# ==============================================================================
def fetch_api_data(endpoint: str) -> list:
    """Fetch JSON data from REST API endpoint"""
    url = f"{API_BASE_URL}{endpoint}"
    response = requests.get(url, timeout=30)
    response.raise_for_status()
    return response.json()


# ==============================================================================
# FLOWGROUP: users_ingestion
# Source: JSONPlaceholder /users API
# ==============================================================================

# Schema for users with nested address and company
users_schema = StructType([
    StructField("id", IntegerType(), True),
    StructField("name", StringType(), True),
    StructField("username", StringType(), True),
    StructField("email", StringType(), True),
    StructField("phone", StringType(), True),
    StructField("website", StringType(), True),
    StructField("address", StructType([
        StructField("street", StringType(), True),
        StructField("suite", StringType(), True),
        StructField("city", StringType(), True),
        StructField("zipcode", StringType(), True),
        StructField("geo", StructType([
            StructField("lat", StringType(), True),
            StructField("lng", StringType(), True)
        ]), True)
    ]), True),
    StructField("company", StructType([
        StructField("name", StringType(), True),
        StructField("catchPhrase", StringType(), True),
        StructField("bs", StringType(), True)
    ]), True)
])


@dlt.view()
def v_api_users_raw():
    """Fetch users data from JSONPlaceholder API"""
    users_data = fetch_api_data("/users")
    return spark.createDataFrame(users_data, schema=users_schema)


@dlt.view()
def v_api_users_flattened():
    """Flatten nested user data structure"""
    df = dlt.read("v_api_users_raw")
    return df.select(
        F.col("id").alias("user_id"),
        F.col("name"),
        F.col("username"),
        F.col("email"),
        F.col("phone"),
        F.col("website"),
        F.col("address.street").alias("address_street"),
        F.col("address.suite").alias("address_suite"),
        F.col("address.city").alias("address_city"),
        F.col("address.zipcode").alias("address_zipcode"),
        F.col("address.geo.lat").cast("double").alias("geo_lat"),
        F.col("address.geo.lng").cast("double").alias("geo_lng"),
        F.col("company.name").alias("company_name"),
        F.col("company.catchPhrase").alias("company_catchphrase"),
        F.col("company.bs").alias("company_bs")
    )


@dlt.view()
def v_api_users_enriched():
    """Add audit metadata to users"""
    df = dlt.read("v_api_users_flattened")
    return df.withColumn("_ingestion_timestamp", F.current_timestamp()) \
             .withColumn("_source_api", F.lit("jsonplaceholder.typicode.com/users")) \
             .withColumn("_record_hash", F.xxhash64(*[F.col(c) for c in df.columns]))


@dlt.table(
    name="api_users",
    comment="Users data from JSONPlaceholder REST API",
    table_properties={
        "delta.autoOptimize.optimizeWrite": "true",
        "quality": "bronze",
        "source": "jsonplaceholder_api"
    }
)
def api_users():
    """Bronze table: API Users"""
    return dlt.read("v_api_users_enriched")


# ==============================================================================
# FLOWGROUP: posts_ingestion
# Source: JSONPlaceholder /posts API
# ==============================================================================

posts_schema = StructType([
    StructField("userId", IntegerType(), True),
    StructField("id", IntegerType(), True),
    StructField("title", StringType(), True),
    StructField("body", StringType(), True)
])


@dlt.view()
def v_api_posts_raw():
    """Fetch posts data from JSONPlaceholder API"""
    posts_data = fetch_api_data("/posts")
    return spark.createDataFrame(posts_data, schema=posts_schema)


@dlt.view()
def v_api_posts_cleaned():
    """Clean and transform posts data"""
    df = dlt.read("v_api_posts_raw")
    return df.select(
        F.col("id").alias("post_id"),
        F.col("userId").alias("user_id"),
        F.col("title"),
        F.col("body").alias("content"),
        F.length(F.col("body")).alias("content_length"),
        F.size(F.split(F.col("body"), " ")).alias("word_count")
    )


@dlt.view()
def v_api_posts_enriched():
    """Add audit metadata to posts"""
    df = dlt.read("v_api_posts_cleaned")
    return df.withColumn("_ingestion_timestamp", F.current_timestamp()) \
             .withColumn("_source_api", F.lit("jsonplaceholder.typicode.com/posts")) \
             .withColumn("_record_hash", F.xxhash64(*[F.col(c) for c in df.columns]))


@dlt.table(
    name="api_posts",
    comment="Blog posts data from JSONPlaceholder REST API",
    table_properties={
        "delta.autoOptimize.optimizeWrite": "true",
        "quality": "bronze",
        "source": "jsonplaceholder_api"
    }
)
def api_posts():
    """Bronze table: API Posts"""
    return dlt.read("v_api_posts_enriched")


# ==============================================================================
# FLOWGROUP: comments_ingestion
# Source: JSONPlaceholder /comments API
# ==============================================================================

comments_schema = StructType([
    StructField("postId", IntegerType(), True),
    StructField("id", IntegerType(), True),
    StructField("name", StringType(), True),
    StructField("email", StringType(), True),
    StructField("body", StringType(), True)
])


@dlt.view()
def v_api_comments_raw():
    """Fetch comments data from JSONPlaceholder API"""
    comments_data = fetch_api_data("/comments")
    return spark.createDataFrame(comments_data, schema=comments_schema)


@dlt.view()
def v_api_comments_cleaned():
    """Clean and transform comments data"""
    df = dlt.read("v_api_comments_raw")
    return df.select(
        F.col("id").alias("comment_id"),
        F.col("postId").alias("post_id"),
        F.col("name").alias("comment_title"),
        F.col("email").alias("commenter_email"),
        F.lower(F.split(F.col("email"), "@")[1]).alias("email_domain"),
        F.col("body").alias("comment_body"),
        F.length(F.col("body")).alias("comment_length")
    )


@dlt.view()
@dlt.expect("valid_comment_id", "comment_id IS NOT NULL")
@dlt.expect("valid_post_id", "post_id IS NOT NULL")
@dlt.expect_or_drop(
    "valid_email",
    "commenter_email RLIKE '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$'"
)
def v_api_comments_validated():
    """Validate comments data with expectations"""
    return dlt.read("v_api_comments_cleaned")


@dlt.view()
def v_api_comments_enriched():
    """Add audit metadata to comments"""
    df = dlt.read("v_api_comments_validated")
    return df.withColumn("_ingestion_timestamp", F.current_timestamp()) \
             .withColumn("_source_api", F.lit("jsonplaceholder.typicode.com/comments")) \
             .withColumn("_record_hash", F.xxhash64(*[F.col(c) for c in df.columns]))


@dlt.table(
    name="api_comments",
    comment="Comments data from JSONPlaceholder REST API",
    table_properties={
        "delta.autoOptimize.optimizeWrite": "true",
        "quality": "bronze",
        "source": "jsonplaceholder_api"
    }
)
def api_comments():
    """Bronze table: API Comments"""
    return dlt.read("v_api_comments_enriched")


# ==============================================================================
# FLOWGROUP: todos_ingestion
# Source: JSONPlaceholder /todos API
# ==============================================================================

todos_schema = StructType([
    StructField("userId", IntegerType(), True),
    StructField("id", IntegerType(), True),
    StructField("title", StringType(), True),
    StructField("completed", StringType(), True)  # API returns string
])


@dlt.view()
def v_api_todos_raw():
    """Fetch todos data from JSONPlaceholder API"""
    todos_data = fetch_api_data("/todos")
    return spark.createDataFrame(todos_data, schema=todos_schema)


@dlt.view()
def v_api_todos_cleaned():
    """Clean and transform todos data"""
    df = dlt.read("v_api_todos_raw")
    return df.select(
        F.col("id").alias("todo_id"),
        F.col("userId").alias("user_id"),
        F.col("title").alias("todo_title"),
        F.when(F.col("completed") == "true", True).otherwise(False).alias("is_completed"),
        F.length(F.col("title")).alias("title_length")
    )


@dlt.view()
def v_api_todos_enriched():
    """Add audit metadata to todos"""
    df = dlt.read("v_api_todos_cleaned")
    return df.withColumn("_ingestion_timestamp", F.current_timestamp()) \
             .withColumn("_source_api", F.lit("jsonplaceholder.typicode.com/todos")) \
             .withColumn("_record_hash", F.xxhash64(*[F.col(c) for c in df.columns]))


@dlt.table(
    name="api_todos",
    comment="Todos data from JSONPlaceholder REST API",
    table_properties={
        "delta.autoOptimize.optimizeWrite": "true",
        "quality": "bronze",
        "source": "jsonplaceholder_api"
    }
)
def api_todos():
    """Bronze table: API Todos"""
    return dlt.read("v_api_todos_enriched")
