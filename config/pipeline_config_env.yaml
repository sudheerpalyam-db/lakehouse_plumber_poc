# ==============================================================================
# LakehousePlumber Pipeline Configuration Template
# ==============================================================================
# 
# This template provides comprehensive configuration options for Delta Live Tables
# (DLT) pipelines when generating Databricks Asset Bundle resource files.
#
# Multi-Document YAML Structure:
#   - First document: project_defaults (applies to ALL pipelines)
#   - Subsequent documents: pipeline-specific configs (override defaults)
#   - Separate documents with '---' (YAML document separator)
#
# Merge Behavior:
#   - Config priority: DEFAULT → project_defaults → pipeline_specific
#   - Nested dicts are deep merged
#   - Lists are REPLACED (not appended)
#
# Usage:
#   1. Copy or rename this file to remove the .tmpl extension
#   2. Customize the settings below for your project
#   3. LHP auto-loads: templates/bundle/pipeline_config.yaml (if exists)
#   4. Or pass explicitly: lhp generate --pipeline-config config/pipeline_config.yaml
#
# Documentation: See docs/databricks_bundles.rst for detailed information
# ==============================================================================

# ==============================================================================
# PROJECT DEFAULTS - Applied to ALL pipelines unless overridden
# ==============================================================================
project_defaults:
  # -----------------------------------------------------------------------------
  # Compute Configuration
  # -----------------------------------------------------------------------------
  
  # Use serverless compute (recommended for most workloads)
  # When true: Databricks manages compute automatically
  # When false: Must specify clusters configuration
  # Default: true
  serverless: true
  
  # DLT Edition - Controls available features and pricing
  # Options: CORE, PRO, ADVANCED
  # Validation: Only these three values are allowed
  # Note: edition parameter only valid when serverless: false
  # Default: ADVANCED
  edition: ADVANCED
  
  # Runtime Channel - Controls DLT runtime version
  # Options: CURRENT (stable), PREVIEW (latest features)
  # Validation: Only these two values are allowed
  # Default: CURRENT
  channel: CURRENT
  
  # -----------------------------------------------------------------------------
  # Processing Mode
  # -----------------------------------------------------------------------------
  
  # Continuous processing (streaming mode)
  # When true: Pipeline runs continuously, processing new data as it arrives
  # When false: Pipeline runs in triggered/batch mode
  # Default: false
  continuous: false
  
  # -----------------------------------------------------------------------------
  # Notifications
  # -----------------------------------------------------------------------------
  
  # Email notifications for pipeline events
  # Applies to all pipelines unless overridden
  # Uncomment and customize as needed
  # notifications:
  #   - email_recipients:
  #       - data-engineering@company.com
  #     alerts:
  #       - on-update-failure
  #       - on-update-fatal-failure
  #       - on-update-success
  #       - on-flow-failure

---
# ==============================================================================
# EXAMPLE: Raw Ingestion Pipeline - Non-serverless with dedicated clusters
# ==============================================================================
# Uncomment and customize for pipelines requiring dedicated compute
# pipeline: raw_customer_ingestions
# 
# # Catalog/Schema Configuration (optional - uses LHP substitution tokens)
# # When defined: Values are directly embedded in bundle resource file
# # When omitted: Uses databricks.yml variables (extracted from Python files)
# # Supports: LHP tokens like "{catalog}" or literal values like "my_catalog"
# catalog: "{catalog}"                # From substitutions/{env}.yaml
# schema: "{raw_schema}"              # Environment-specific schema
# 
# serverless: false
# edition: PRO
# continuous: true
# 
# # Cluster configuration (required when serverless: false)
# clusters:
#   - label: default
#     node_type_id: Standard_D16ds_v5
#     driver_node_type_id: Standard_D32ds_v5
#     autoscale:
#       min_workers: 2
#       max_workers: 10
#       mode: ENHANCED
#     # Optional: Cluster policy ID for governance
#     policy_id: YOUR_POLICY_ID_HERE
# 
# # Event logging configuration
# event_log:
#   name: raw_ingestion_event_log
#   schema: _meta
#   catalog: ${var.default_pipeline_catalog}
#
# # Pipeline-level permissions
# permissions:
#   - service_principal_name: 2aa4ed8e-0a18-4072-97c6-9c074c8be40d
#     level: CAN_MANAGE
#   - user_name: user@example.com
#     level: CAN_RUN
#   - group_name: data-engineers
#     level: CAN_VIEW

# # Pipeline-level run_as
# run_as:
#   service_principal_name: 2aa4ed8e-0a18-4072-97c6-9c074c8be40d

---
# ==============================================================================
# EXAMPLE: Bronze Layer Pipeline - Continuous streaming with serverless
# ==============================================================================
# pipeline: bronze_load
# 
# # Catalog/Schema: Token-based (environment-specific)
# catalog: "{catalog}"
# schema: "{bronze_schema}"
# 
# serverless: true
# continuous: true
# edition: ADVANCED
# 
# # Override project default notifications
# notifications:
#   - email_recipients:
#       - bronze-team@company.com
#     alerts:
#       - on-update-failure
#       - on-flow-failure
# 
# # Custom tags for cost tracking and organization
# tags:
#   layer: bronze
#   criticality: high
#   sla: 30min

---
# ==============================================================================
# EXAMPLE: Silver Layer Pipeline - Batch processing with advanced features
# ==============================================================================
# pipeline: silver_transformations
# 
# # Catalog/Schema: Environment-specific
# catalog: "{catalog}"
# schema: "{silver_schema}"
# 
# serverless: true
# continuous: false
# edition: ADVANCED
# channel: CURRENT
# 
# # Photon engine (only for non-serverless)
# # photon: true
# 
# # Pipeline-specific notifications
# notifications:
#   - email_recipients:
#       - silver-team@company.com
#       - data-quality@company.com
#     alerts:
#       - on-update-failure
#       - on-flow-failure
# 
# # Custom tags
# tags:
#   layer: silver
#   criticality: medium
#   contains_pii: true
# 
# # Event log with custom configuration
# event_log:
#   name: silver_event_log
#   schema: _meta
#   catalog: ${var.default_pipeline_catalog}

---
# ==============================================================================
# EXAMPLE: Gold Layer Pipeline - Production-ready with strict monitoring
# ==============================================================================
# pipeline: gold_aggregations
# 
# # Catalog/Schema: EXAMPLE - Mix of literal and token
# # Literal catalog: Fixed value across all environments  
# # Token schema: Varies by environment (dev/staging/prod)
# catalog: "analytics_prod"           # Literal - always analytics_prod
# schema: "{gold_schema}"              # Token - from substitutions file
# 
# serverless: true
# continuous: false
# edition: ADVANCED
# 
# # Comprehensive notifications for production pipeline
# notifications:
#   - email_recipients:
#       - gold-team@company.com
#       - data-ops@company.com
#       - business-analytics@company.com
#     alerts:
#       - on-update-failure
#       - on-update-fatal-failure
#       - on-update-success
#       - on-flow-failure
# 
# # Production tags
# tags:
#   layer: gold
#   criticality: critical
#   sla: 15min
#   business_critical: true
# 
# # Event log for audit and monitoring
# event_log:
#   name: gold_event_log
#   schema: _meta
#   catalog: ${var.default_pipeline_catalog}

---
# ==============================================================================
# EXAMPLE: Full Substitution - ALL Fields Support Tokens
# ==============================================================================
# LHP token substitution works for ALL fields, not just catalog/schema!
# This enables complete environment-specific configuration.
#
# pipeline: production_ingestion
# 
# # Catalog/schema with tokens
# catalog: "{catalog}"
# schema: "{raw_schema}"
# serverless: false
# 
# # Environment-specific node types and policies
# clusters:
#   - label: default
#     node_type_id: "{pipeline_node_type}"    # ← Dev: smaller, Prod: larger
#     policy_id: "{pipeline_policy_id}"        # ← Different policies per env
#     autoscale:
#       min_workers: 2
#       max_workers: 10
# 
# # Environment-specific notification emails
# notifications:
#   - email_recipients:
#       - "{ops_team_email}"                   # ← Resolves to dev-ops@ or ops@
#       - "{data_eng_email}"                   # ← Resolves to team email per env
#     alerts:
#       - on-update-failure
#       - on-update-fatal-failure
# 
# # Dynamic tagging based on environment
# tags:
#   environment: "{environment_name}"          # ← development, staging, production
#   cost_center: "{cost_center_code}"          # ← Different codes per env
#   team: data-engineering                     # ← Literal value
# 
# # Event logging with tokens
# event_log:
#   name: pipeline_events
#   schema: _meta
#   catalog: "{event_log_catalog}"             # ← dev_meta, prod_meta, etc.
#
# Corresponding substitutions/dev.yaml:
#   dev:
#     catalog: acme_dev
#     raw_schema: raw_dev
#     pipeline_node_type: Standard_D8ds_v5     # Smaller for dev
#     pipeline_policy_id: dev-policy-123
#     ops_team_email: dev-ops@company.com
#     data_eng_email: dev-team@company.com
#     environment_name: development
#     cost_center_code: CC-DEV-001
#     event_log_catalog: dev_meta
#
# Corresponding substitutions/prod.yaml:
#   prod:
#     catalog: acme_prod
#     raw_schema: raw_prod
#     pipeline_node_type: Standard_D32ds_v5    # Larger for prod
#     pipeline_policy_id: prod-policy-456
#     ops_team_email: ops@company.com
#     data_eng_email: data-eng@company.com
#     environment_name: production
#     cost_center_code: CC-PROD-001
#     event_log_catalog: prod_meta
#
# ==============================================================================
# Configuration Notes
# ==============================================================================
#
# Full Substitution Support (NEW):
#   - ALL fields in pipeline_config.yaml support LHP token substitution
#   - Use tokens for: node_type_id, policy_id, email_recipients, tags, etc.
#   - Enables complete environment-specific configuration
#   - Tokens are resolved from substitutions/{env}.yaml files
#   - Unresolved tokens pass through unchanged (not an error)
#   - Missing substitution file: uses raw config without substitution
#
# Catalog/Schema Configuration:
#   - Define catalog and schema at pipeline level to override default behavior
#   - Must define BOTH catalog AND schema (defining only one raises an error)
#   - Supports LHP substitution tokens: "{catalog}", "{schema}", etc.
#   - Supports literal values: "my_catalog", "my_schema"
#   - Values are resolved from substitutions/{env}.yaml files
#   - When defined: Values embedded directly in bundle resource file
#   - When omitted: Uses ${var.default_pipeline_catalog} from databricks.yml
#
# Key Constraints:
#   - edition parameter only works when serverless: false
#   - photon parameter only works when serverless: false
#   - clusters configuration required when serverless: false
#   - tags only work for non-serverless pipelines (serverless uses compute policy)
#   - catalog and schema: Must define both or neither (not just one)
#
# Validation Rules:
#   - edition: Must be CORE, PRO, or ADVANCED
#   - channel: Must be CURRENT or PREVIEW
#   - catalog/schema: Both required if either is defined
#   - Empty catalog/schema values raise errors
#   - Unresolved tokens raise errors
#   - Lists (notifications, clusters, tags) are REPLACED during merge, not appended
#
# Best Practices:
#   - Use serverless for most workloads (cost-effective, auto-scaling)
#   - Use dedicated clusters for large-scale continuous ingestion
#   - Set appropriate notifications based on pipeline criticality
#   - Use tags for cost tracking and organization (non-serverless only)
#   - Use catalog/schema config when pipelines target different Unity Catalog locations
#   - Test configurations in dev before deploying to production
#
# For more information:
#   - See docs/databricks_bundles.rst
#   - Refer to Databricks DLT documentation
#
# ==============================================================================

